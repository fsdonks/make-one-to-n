{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAA Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Capacity Analysis Run with Default Initial Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demandtrends folder\n",
    "root=\"/home/craig/runs/big_test/base-testdata-v7/\"\n",
    "dtrends = root+ \"DemandTrends.txt\"\n",
    "df=pd.read_csv(dtrends, sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot a line chart of TotalRequired and Deployed, we group by time and sum the values so that we have the total TotalRequired and Deployed for each day.  If you don't reset_index, you get a multi-index dataframe from groupby, which you can't plot, but functions called on groupby (like sum() here) will sum the values in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = df.groupby(['t']).sum().reset_index()\n",
    "group_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot('t', 'TotalRequired', data=group_df)\n",
    "plt.plot('t', 'Deployed', data=group_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initial Conditions Output Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been storing the results the the parent directory alongside the MARATHON workbook.  results.txt is from random initial condition runs from marathon.analysis.random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = root+ \"../results.txt\" \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(results, sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we count the number records for each \\[SRC, AC\\] group.  For x initial condition reps and y phases, we should have x*y records.  This is essentially pivoting in Python by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = df.groupby(by=['SRC', 'AC']).count().reset_index()\n",
    "group_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any \\[SRC, AC\\] tuple that doesn't have x*y records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df[group_df['rep-seed']!=12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to compute Score and Excess for each \\[SRC, AC\\] tuple.  \n",
    "\n",
    "First, average NG fill, then average RC fill, then average NG fill, then sum and divide by demand for Score (note that fill is fill from demandtrends and NOT just deployed like the field was renamed in 2327)\n",
    "Excess is sum of available for each component divided by demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dmet='demand_met'\n",
    "emet='excess_met'\n",
    "#returns % excess demand met for every SRC, AC, phase combination\n",
    "#used once to find the max and once for actually dataframe computation\n",
    "def compute_excess(in_df):\n",
    "    in_df[emet]=(in_df['NG-deployable'] + in_df['AC-deployable'] + in_df['RC-deployable']) / in_df['total-quantity']\n",
    "    \n",
    "import numpy as np\n",
    "#compute % demand met (dmet) and % excess over the demand (emet) \n",
    "#first by phase (use average group by with src, ac, phase)\n",
    "def by_phase_percentages(results_df):\n",
    "    group_df = results_df.groupby(by=['SRC', 'AC', 'phase']).mean().reset_index()\n",
    "    #when there is no demand in a phase, dmet is 100%\n",
    "    group_df[dmet] = np.where((group_df['total-quantity']==0), 1, \n",
    "                                                                (group_df['NG-fill'] + \n",
    "                                                                group_df['AC-fill'] + \n",
    "                                                                group_df['RC-fill']) / group_df['total-quantity'])\n",
    "    #When there is no demand in a phase, emet is the max emet across all SRCs and phases.\n",
    "    excess_df = copy.deepcopy(group_df[(group_df['total-quantity'] != 0)])\n",
    "    compute_excess(excess_df)\n",
    "    max_excess=excess_df[emet].max()+1\n",
    "    \n",
    "    group_df[emet] = np.where((group_df['total-quantity']==0), max_excess, \n",
    "                                                        (group_df['NG-deployable'] + \n",
    "                                                        group_df['AC-deployable'] + \n",
    "                                                        group_df['RC-deployable']) / group_df['total-quantity'])\n",
    "    print(group_df['total-quantity'].isnull().sum())\n",
    "    #this will be 0 because if there is no demand, we don't have a record.\n",
    "\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do first: 1 workbook\n",
    "\t(need to groupby.mean.unstack phase, but what do I expect?)\n",
    "\tTab 1: src, ac, results by phase for demand 1, add score, excess\n",
    "\tTab 2: src, ac, results by phase in columns for demand 2, add score excess\n",
    "\tTab 3: src, ac, score-demand1, excess-demand1, score-dmd2, excess-dmd2, min-demand, min score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce \n",
    "\n",
    "def results_by_phase(results_df):\n",
    "    res=results_df.groupby(by=['SRC', 'AC', 'phase']).mean()\n",
    "    return res.unstack(level=['phase'])\n",
    "\n",
    "#Weights used for a weighted score.\n",
    "phase_weights= {\"comp1\" : 0.125,\n",
    "               \"comp2\" : 0.125,\n",
    "               \"phase1\" : .0625,\n",
    "               \"phase2\" : .0625,\n",
    "               \"phase3\" : .5,\n",
    "               \"phase4\" : .125}\n",
    "\n",
    "d_weighted = 'dmet_times_weight'\n",
    "e_weighted = 'emet_times_weight'\n",
    "dmet_sum='weighted_dmet_sum'\n",
    "emet_sum='weighted_emet_sum'\n",
    "#given an ordered list of initial columns, put the rest of the columns in the dataframe at the end\n",
    "def reorder_columns(order, df):\n",
    "    cols=[c for c in order if c in df] + [c for c in df if c not in order]\n",
    "    return df[cols]\n",
    "\n",
    "#compute score and excess from a path to results.txt\n",
    "def compute_scores(results_path):\n",
    "    df=pd.read_csv(results_path, sep='\\t')\n",
    "    #sometimes all inventory was equal to 0, but we shouldn't have that. \n",
    "    #We should have all phases if all inventory ==0\n",
    "    df= df[(df[['AC', 'NG', 'RC']] == 0).all(axis=1)==False]\n",
    "    scores = by_phase_percentages(df)\n",
    "    scores['weight']=scores['phase'].map(phase_weights)\n",
    "    scores[d_weighted]=scores[dmet]*scores['weight']\n",
    "    scores[e_weighted]=scores[emet]*scores['weight']\n",
    "    res = results_by_phase(scores[['SRC', 'AC', 'NG', 'RC', \n",
    "                                   'phase', dmet, emet, \n",
    "                                   'weight', d_weighted,\n",
    "                                  e_weighted]])\n",
    "    res[('Score', dmet_sum)]=res.iloc[:, res.columns.get_level_values(0)==d_weighted].sum(axis=1)\n",
    "    res[('Excess', emet_sum)]=res.iloc[:, res.columns.get_level_values(0)==e_weighted].sum(axis=1)\n",
    "    res[('NG_inv', '')]=res.iloc[:, res.columns.get_level_values(0)=='NG'].max(axis=1)\n",
    "    res[('RC_inv', '')]=res.iloc[:, res.columns.get_level_values(0)=='RC'].max(axis=1)\n",
    "    #need to join multindex columns to single index columns in title_strength, so this the merge process\n",
    "    tuples = [('SRC', ''), ('TITLE', ''), ('STR', '')]\n",
    "    titles=copy.deepcopy(title_strength)\n",
    "    titles.columns=pd.MultiIndex.from_tuples(tuples, names=(None, 'phase'))\n",
    "    res = pd.merge(res.reset_index(),\n",
    "          titles,\n",
    "          on=[('SRC', '')],\n",
    "          how='inner'\n",
    "         ).set_index(['SRC', 'AC'])\n",
    "    res.drop(['NG', 'RC'], axis=1, level=0, inplace=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the SRC baseline for strength and OI title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = pd.read_excel('/home/craig/runs/big_test/TAA24-28_SRC_BASELINE_201130_DRAFTv6.xlsx', sheet_name='SRC_Baseline TAA 24-28')\n",
    "title_strength=baseline[['SRC', 'TITLE', 'STR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "results_list = [\"/home/craig/runs/big_test/results (copy 1).txt\", \"/home/craig/runs/big_test/results.txt\"]\n",
    "results_map = {'2.1' : \"/home/craig/runs/big_test/results_no_truncate_and_1_supply.txt\", \n",
    "               '7.1' : \"/home/craig/runs/big_test/results.txt\",\n",
    "              '10.1' : \"/home/craig/runs/big_test/results_no_truncation.txt\"}\n",
    "\n",
    "writer = pd.ExcelWriter('TAA24-28_Modeling_Results.xlsx', engine='xlsxwriter')\n",
    "left=pd.DataFrame()\n",
    "\n",
    "for demand_name in results_map:\n",
    "    scored_results = compute_scores(results_map[demand_name])\n",
    "    if left.empty:\n",
    "        max_df=scored_results.reset_index().groupby('SRC')['AC'].apply(max)\n",
    "        maxes=max_df.to_dict()\n",
    "    #just to repeat the SRC in the output. Also will add an index on the left.\n",
    "    scored_results.reset_index(inplace=True)\n",
    "    #add max ac inventory\n",
    "    scored_results['max_AC_inv']=scored_results['SRC'].map(maxes)\n",
    "    #filter out the base inventories\n",
    "    scored_results=scored_results[scored_results['AC']!=scored_results['max_AC_inv']]\n",
    "    #add one to the remaining inventory records\n",
    "    scored_results['AC']=scored_results['AC']+1\n",
    "    #indicate those records that are the base supply\n",
    "    scored_results['base_supply']=np.where((scored_results['AC']==scored_results['max_AC_inv']), 'X', 'Down')\n",
    "    #remove maxes\n",
    "    scored_results.drop(columns=['max_AC_inv'], level=0, inplace=True)\n",
    "    #add scores to all_scores\n",
    "    scored_results=scored_results.set_index(['SRC', 'AC'])\n",
    "    score_columns=[('Score', dmet_sum), ('Excess', emet_sum)]\n",
    "    score_col_names=['Score_'+demand_name, 'Excess_'+demand_name]\n",
    "    #if left.empty:\n",
    "        #score_columns=[('TITLE', '')] + score_columns\n",
    "        #score_col_names = ['TITLE']+score_col_names\n",
    "    scores = scored_results[score_columns]\n",
    "    scores.columns=score_col_names\n",
    "    if left.empty:\n",
    "        left=scores\n",
    "    else:\n",
    "        right=scores\n",
    "        left = pd.merge(left,right,on=['SRC', 'AC'], how='inner')\n",
    "    #write to excel file here\n",
    "    scored_results.reset_index(inplace=True)\n",
    "    scored_results.rename(columns={'NG_inv':'NG', 'RC_inv':'AR', 'AC':'RA'}, inplace=True, level=0)\n",
    "    initial_cols = [('SRC', ''), ('TITLE', ''), ('RA', ''), ('NG', ''), \n",
    "                    ('AR', ''),\n",
    "                   ]\n",
    "    reordered=reorder_columns(initial_cols, scored_results)\n",
    "    reordered.to_excel(writer, sheet_name=demand_name) \n",
    "left.reset_index(inplace=True)\n",
    "\n",
    "left['min_score']=left[['Score_'+demand_name for demand_name in results_map]].min(axis=1)\n",
    "left['min_score_demand'] = left[['Score_'+demand_name for demand_name in results_map]].idxmin(axis=1)\n",
    "#write third worksheet with all scores here\n",
    "left = pd.merge(left, title_strength, on='SRC')\n",
    "left = reorder_columns(['SRC', 'TITLE', 'AC'], left)\n",
    "left.to_excel(writer, sheet_name='all_scores')\n",
    "writer.save()\n",
    "\n",
    "\n",
    "#Given a cell in a sheet starting at row row_start and in column,clear all cell contents\n",
    "def clear_column(row_start, column, sh):\n",
    "    for row in range(row_start,sh.max_row):\n",
    "        if(sh.cell(row,column).value is  None):\n",
    "            break\n",
    "        sh.cell(row,column).value= None\n",
    "\n",
    "wb = openpyxl.reader.excel.load_workbook('TAA24-28_Modeling_Results.xlsx')\n",
    "ws=wb['all_scores']\n",
    "#clear_column(2, 1, ws)\n",
    "ws.delete_cols(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing the multi-index dataframes to Excel, pandas put an extra blank row below the column names, which messes up the filter in LibreOffice, but not Excel.  In Excel, you could turn the filter on the blank row.  In LibreOffice, that didn't work.  Although, in LibreOffice, you can turn it on the first row and it captures the first value.  Excel does not.  So those are the filter workarounds, but it looks cleaner to just remove that blank row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for demand_name in results_map:\n",
    "    sh = wb[demand_name]\n",
    "    sh.delete_rows(3, 1)\n",
    "    #We don't want index to show, and can't do with multi-index to_excel yet, so have to do it manually\n",
    "    clear_column(3, 1, sh)\n",
    "wb.save('TAA24-28_Modeling_Results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#join tables so that you have two score columns\n",
    "#add column called min_score\n",
    "#add another column called min_score_demand\n",
    "#could turn this into a map to concat both demand tables then, but not necessary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
