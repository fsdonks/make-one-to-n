{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAA Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Capacity Analysis Run with Default Initial Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demandtrends folder\n",
    "root=\"/home/craig/runs/big_test/base-testdata-v7/\"\n",
    "dtrends = root+ \"DemandTrends.txt\"\n",
    "df=pd.read_csv(dtrends, sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot a line chart of TotalRequired and Deployed, we group by time and sum the values so that we have the total TotalRequired and Deployed for each day.  If you don't reset_index, you get a multi-index dataframe from groupby, which you can't plot, but functions called on groupby (like sum() here) will sum the values in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = df.groupby(['t']).sum().reset_index()\n",
    "group_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot('t', 'TotalRequired', data=group_df)\n",
    "plt.plot('t', 'Deployed', data=group_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initial Conditions Output Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been storing the results the the parent directory alongside the MARATHON workbook.  results.txt is from random initial condition runs from marathon.analysis.random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = root+ \"../results.txt\" \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(results, sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we count the number records for each \\[SRC, AC\\] group.  For x initial condition reps and y phases, we should have x*y records.  This is essentially pivoting in Python by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = df.groupby(by=['SRC', 'AC']).count().reset_index()\n",
    "group_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any \\[SRC, AC\\] tuple that doesn't have x*y records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df[group_df['rep-seed']!=12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to compute Score and Excess for each \\[SRC, AC\\] tuple.  \n",
    "\n",
    "First, average NG fill, then average RC fill, then average NG fill, then sum and divide by demand for Score (note that fill is fill from demandtrends and NOT just deployed like the field was renamed in 2327)\n",
    "Excess is sum of available for each component divided by demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#compute % demand met (dmet) and % excess over the demand (emet) \n",
    "#first by phase (use average group by with src, ac, phase)\n",
    "def by_phase_percentages(results_df):\n",
    "    group_df = results_df.groupby(by=['SRC', 'AC', 'phase']).mean().reset_index()\n",
    "    group_df['dmet'] = np.where((group_df['total-quantity']==0), 1, \n",
    "                                                                (group_df['NG-fill'] + \n",
    "                                                                group_df['AC-fill'] + \n",
    "                                                                group_df['RC-fill']) / group_df['total-quantity'])\n",
    "    #blaaah. what do I do?, this should be max for the phase instead.\n",
    "    group_df['emet'] = np.where((group_df['total-quantity']==0), 1, \n",
    "                                                        (group_df['NG-deployable'] + \n",
    "                                                        group_df['AC-deployable'] + \n",
    "                                                        group_df['RC-deployable']) / group_df['total-quantity'])\n",
    "    print(group_df['total-quantity'].isnull().sum())\n",
    "    #this will be 0 because if there is no demand, we don't have a record.\n",
    "    group_df.head()\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do first: 1 workbook\n",
    "\t(need to groupby.mean.unstack phase, but what do I expect?)\n",
    "\tTab 1: src, ac, results by phase for demand 1, add score, excess\n",
    "\tTab 2: src, ac, results by phase in columns for demand 2, add score excess\n",
    "\tTab 3: src, ac, score-demand1, excess-demand1, score-dmd2, excess-dmd2, min-demand, min score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_SRC_AC(table_list):\n",
    "    return reduce(lambda  left,right: pd.merge(left,right,on=['SRC', 'AC'], \n",
    "                                            how='inner'), table_list)\n",
    "def merge_by_SRC_AC2(table_list):\n",
    "    return reduce(lambda  left,right: left.join(right,on=['SRC', 'AC'],\n",
    "                                            how='inner'), table_list)\n",
    "\n",
    "#just a place holder in the output table, was empty '' before\n",
    "score_phase = 'combined'\n",
    "def merge_two(left, right):\n",
    "    #need to make the right table have a phase level in order to join it on right\n",
    "    #we also want to name the Score and Excess phases combined so that our\n",
    "    #our LibreCalc filters work properly.\n",
    "    tuples = [('SRC', ''), ('AC', ''), ('Score', score_phase), ('Excess', score_phase)]\n",
    "    right.columns=pd.MultiIndex.from_tuples(tuples, names=(None, 'phase'))\n",
    "    return left.join(right.set_index(['SRC', 'AC']), how='inner')\n",
    "\n",
    "def merge_by_SRC_AC3(table_list):\n",
    "    return reduce(merge_two, table_list)\n",
    "                                                \n",
    "def results_by_phase(results_df):\n",
    "    return results_df.groupby(by=['SRC', 'AC', 'phase']).mean().unstack(level=['phase'])\n",
    "\n",
    "columns_by_phase=results_by_phase(df)\n",
    "columns_by_phase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce \n",
    "\n",
    "#Weights used for a weighted score.\n",
    "phase_weights= {\"comp1\" : 0.125,\n",
    "               \"comp2\" : 0.125,\n",
    "               \"phase1\" : .0625,\n",
    "               \"phase2\" : .0625,\n",
    "               \"phase3\" : .5,\n",
    "               \"phase4\" : .125}\n",
    "\n",
    "#add the weight to each row\n",
    "def row_weight(row):\n",
    "    return phase_weights[row['phase']]\n",
    "\n",
    "#then group by src, ac, using custom function for weighted phases\n",
    "def weighted_average(df, data_col, weight_col, by_col):\n",
    "    df['weight']=df.apply(lambda row: row_weight(row), axis=1)\n",
    "    df['_data_times_weight'] = df[data_col] * df[weight_col]\n",
    "    g = df.groupby(by=by_col)\n",
    "    #note that if we're missing a phase, the weight is adjusted accordingly\n",
    "    #change: assume weight is 1 for all.\n",
    "    res = g['_data_times_weight'].sum()  #/ g[weight_col].sum()\n",
    "    print(res)\n",
    "    del df['_data_times_weight']\n",
    "    return res\n",
    "\n",
    "#compute score and excess from demand met and demand excess\n",
    "def by_src_inventory_scores(percentages_df):\n",
    "    inventory_score = weighted_average(percentages_df, 'dmet', 'weight', ['SRC', 'AC']).to_frame()\n",
    "    inventory_excess = weighted_average(percentages_df, 'emet', 'weight', ['SRC', 'AC']).to_frame()\n",
    "    print(inventory_score.reset_index())\n",
    "    res_df=inventory_score.reset_index().rename(columns={inventory_score.columns[0] : \"Score\"})\n",
    "    print(res_df)\n",
    "    #join both Score and Excess results.\n",
    "    res_df = res_df.merge(inventory_excess.reset_index().rename(columns={inventory_excess.columns[0] : \"Excess\"}), \n",
    "                        how='inner', on=['SRC', 'AC'])\n",
    "    return res_df\n",
    "\n",
    "#compute score and excess from a path to results.txt\n",
    "def by_src_ac_scores(results_path):\n",
    "    df=pd.read_csv(results_path, sep='\\t')\n",
    "    results_df = results_by_phase(df)\n",
    "    scores = by_src_inventory_scores(by_phase_percentages(df))\n",
    "    return merge_by_SRC_AC3([results_df, scores])\n",
    "\n",
    "by_src_ac_scores(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have a way to take a list of paths to results.txts and put them in one table, add a column called min_score, add another column called min_score_path to indicate the path with the min score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "results_list = [\"/home/craig/runs/big_test/results (copy 1).txt\", \"/home/craig/runs/big_test/results.txt\"]\n",
    "results_map = {'2.1' : \"/home/craig/runs/big_test/results (copy 1).txt\", \n",
    "               '7.1' : \"/home/craig/runs/big_test/results.txt\"}\n",
    "score_list = map(lambda results: by_src_ac_scores(results), results_list) \n",
    "#need to check out this merge next...\n",
    "'''df_merged = merge_by_SRC_AC(score_list)\n",
    "df_merged=df_merged.rename(columns=\n",
    "                                {'Score_x':'Score_7',\n",
    "                                 'Excess_x':'Excess_7',\n",
    "                                 'Score_y':'Score_2',\n",
    "                                 'Excess_y':'Excess_2'})\n",
    "df_merged.head()'''\n",
    "\n",
    "writer = pd.ExcelWriter('TAA24-28_Modeling_Results.xlsx', engine='xlsxwriter')\n",
    "left=pd.DataFrame()\n",
    "for demand_name in results_map:\n",
    "    scored_results = by_src_ac_scores(results_map[demand_name])\n",
    "    print(scored_results)\n",
    "    \n",
    "    scores = scored_results[[('Score', score_phase), ('Excess', score_phase)]]\n",
    "    scores.columns=['Score_'+demand_name, 'Excess_'+demand_name]\n",
    "    if left.empty:\n",
    "        left=scores\n",
    "    else:\n",
    "        right=scores\n",
    "        left = pd.merge(left,right,on=['SRC', 'AC'], how='inner')\n",
    "    scored_results.reset_index(inplace=True)\n",
    "    #write to excel file here\n",
    "    scored_results.to_excel(writer, sheet_name=demand_name)\n",
    "left.head()  \n",
    "\n",
    "#write third worksheet here\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing the multi-index dataframes to Excel, pandas put an extra blank row below the column names, which messes up the filter in LibreOffice, but not Excel.  In Excel, you could turn the filter on the blank row.  In LibreOffice, that didn't work.  Although, in LibreOffice, you can turn it on the first row and it captures the first value.  Excel does not.  So those are the filter workarounds, but it looks cleaner to just remove that blank row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = openpyxl.reader.excel.load_workbook('TAA24-28_Modeling_Results.xlsx')\n",
    "for demand_name in results_map:\n",
    "    sh = wb.get_sheet_by_name(demand_name)\n",
    "    sh.delete_rows(3, 1)\n",
    "wb.save('TAA24-28_Modeling_Results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#join tables so that you have two score columns\n",
    "#add column called min_score\n",
    "#add another column called min_score_demand\n",
    "#could turn this into a map to concat both demand tables then, but not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xlsxwriter as writer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
